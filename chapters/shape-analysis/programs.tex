%% ====================================================================
 
\chapter{Data Structures}
\label{section:shape:programs}
\index{Program Model}%
%% ====================================================================
\index{Concurrency}%
                
%\begin{figure}
%\center
%\begin{tikzpicture}[]
%\node[rounded corners,draw = cyan,name=cell8,minimum width=24pt, minimum height=20pt,anchor=south]{};
%\node[minimum width=8pt, minimum height=10pt,anchor=north west,font=\tiny,inner sep=0pt] at (cell8.north west){};
%\node[ellipse callout, fill=yellow!20,anchor=east,inner sep=2pt, 
%callout absolute pointer={($(cell8.north west)+(4pt,-5pt)$)},draw,,font=\tiny,align=center]  
%at ($(cell8.north west)+(-4pt,-2pt)$){abstract\\ value};
%%
%\node[minimum width=8pt, minimum height=10pt,anchor=south west,font=\tiny,inner sep=0pt] at (cell8.south west){};
%\node[ellipse callout, fill=yellow!20,anchor=east,inner sep=2pt, 
%callout absolute pointer={($(cell8.south west)+(4pt,5pt)$)},draw,,font=\tiny,align=center]  
%at ($(cell8.south west)+(-4pt,2pt)$){concrete\\ value};
%%
%%
%\node[draw = blue!50,minimum width=8pt, minimum height=10pt,anchor=north,font=\tiny,inner sep=0pt]at (cell8.north){};
%\node[ellipse callout, fill=yellow!20,inner sep=2pt, 
%callout absolute pointer={($(cell8.north)+(0pt,-5pt)$)},draw,,font=\tiny,align=center]  
%at ($(cell8.north)+(0pt,7pt)$){{\tt mark}};
%%  
%\node[draw = cyan,minimum width=8pt, minimum height=10pt,anchor=south,font=\tiny,inner sep=0pt]at (cell8.south){};
%\node[ellipse callout, fill=yellow!20,inner sep=2pt, 
%callout absolute pointer={($(cell8.south)+(0pt,5pt)$)},draw,,font=\tiny,align=center]  
%at ($(cell8.south)+(0pt,-7pt)$){{\tt lock}};
%%
%\node[ellipse callout, fill=yellow!20,inner sep=2pt, 
%callout absolute pointer={($(cell8.east)+(-4pt,0pt)$)},draw,,font=\tiny,align=center]  
%at ($(cell8.east)+(13pt,0pt)$){{\tt next}};
%%
%%
%\node[rounded corners,draw = cyan,name=cell1,minimum width=24pt, minimum height=20pt,anchor=west] at ($(cell8.east)+(-50pt,-40pt)$){};
%\node[font=\tiny,inner sep=0pt,scale=0.8,anchor=west] at ($(cell1.west)+(0.5pt,1pt)$){{--$\infty$}};
%\node[draw = blue!50,minimum width=8pt, minimum height=10pt,anchor=north,font=\tiny,inner sep=0pt]at (cell1.north){};
%\node[draw = cyan,minimum width=8pt, minimum height=10pt,anchor=south,font=\tiny,inner sep=0pt]at (cell1.south){\cross};
%\node[name=succ1,circle,fill,minimum size=3pt,inner sep=0pt,outer sep=0pt] at ($(cell1.east)+(-4pt,0pt)$) {};
%\node[anchor=north,font=\tiny,align=center] at ($(cell1.south)+(0pt,2pt)$) {{\tt head}};
%%
%\node[rounded corners,draw = cyan,name=cell6,minimum width=24pt, minimum height=20pt,anchor=west,name=cell6]
%at ($(cell1.east)+(10pt,0pt)$){};
%\node[minimum width=8pt, minimum height=10pt,anchor=west,font=\tiny,inner sep=0pt,scale=0.8,name=d6a] at ($(cell6.west)+(1pt,1pt)$){$\infty$};
%\node[draw = blue!50,minimum width=8pt, minimum height=10pt,anchor=north,font=\tiny,inner sep=0pt]at (cell6.north){\cross};
%\node[draw = cyan,minimum width=8pt, minimum height=10pt,anchor=south,font=\tiny,inner sep=0pt]at (cell6.south){\cross};
%\draw ($(cell6.north east)+(-1pt,-2pt)$) -- ($(cell6.south east)+(-8pt,0pt)$);
%\draw ($(cell6.north east)+(-8pt,0pt)$) -- ($(cell6.south east)+(-1pt,2pt)$);
%\draw[->] (succ1) -- (cell6);
%\node[anchor=south,font=\tiny,align=center] at ($(cell6.north)+(0pt,-2pt)$) {{\tt tail}};
%\end{tikzpicture}
%\caption{A cell and the initial heap in the {\tt Lazy Set} Algorithm.}
%\label{lazy:list:heap:cell:fig}
%\end{figure}
In general, a data structure is any data representation and its
associated operations. Even an integer or floating point number stored on the computer can be viewed as a simple data structure. Typically, a data structure is meant to be an organization or structuring for a collection of data items. 
%A sorted list of integers stored in an array is an example of such a structuring. 
%The implementation of a data structure consists of a set of operations that access and manipulate instances of that structure concurrently.
 Each data structure has an interface which defines a set of possible values and a set of operations. 
 %Examples of data structures are queue, stack and set. 
 More precisely, the interface consists of a set of operations or methods, each having a number of input and output parameters, and a specification of the effect of each operation. 
%Each data structure have its own specification that specify behaviors of its operations. The behavior of each operation is determined by its inputs and outputs. 
%For example, the behavior of a stack object can be specified by requiring that its {\tt push} operation insert an item in the stack, and that {\tt pop} removes the newest item present in the stack. Data structure typically used heap-allocated memory to store their data. A heap-allocated memory can be singly-linked list, skip-list or array of lists. 
%A stack has two operations {\tt push} and {\tt pop}. A {\tt push} operation inserts an element $\tt e$ which is the parameter of {\tt push} in the stack, and {\tt pop} removes the newest item present in the stack. 

For example,  a sequential set is an data structure for storing a collection of elements, with the three operations as following: 
\begin{itemize}
\item $\tt add(e)$ adds element $\tt e$ into the set, returning \true\; if, and only if $\tt e$ was not
already there. 
\item $\tt remove(e)$ removes element $\tt e$ from the set, returning \true\; if, and only if
$\tt e$ was there. 
\item $\tt contains(e)$ checks the existence of element $\tt e$ in the set, returns \true\; if, and only if the set contains $\tt e$. 
\end{itemize}
For each method, we say that a call is successful if it returns true, and unsuccessful
otherwise. It is typical that in applications using sets, there are significantly more
$\tt contains()$ calls than $\tt add()$ or $\tt remove()$ calls.
A set is implemented as a singly linked list of cells. Each cell has two fields. The $\tt val$ field present value of a cell. Cells are sorted according to $\tt val$ order, providing an efficient way to detect when an item is absent. The $\tt next$ field is a reference to
the next cell in the list. The list has two sentinel cells, called head and tail, which are first and last list elements. Sentinel nodes are never added, removed, or searched for, and their values are the minimum and maximum integer values.

The implementation of a data structure should provide
an efficient way to store data in computer memory and perform its operations
in an efficient way. Data structures
typically used heap-allocated memory to store their data. Various schemes
can be used to organize the heap-allocated memory, such as singly-linked lists,
doubly-linked lists, skip-lists, trees. The implementation
can be sequential or concurrent.

%A concurrent data structure is a way of storing and organizing data for access and manipulation by multiple computing threads (or processes) on a shared-memory computer. Each operation is implemented as a sequential
%method that is executed by a thread. Several features of shared-memory multiprocessors make concurrent data structures significantly more difficult to design and to verify as correct than their sequential counterparts. The primary source of this additional difficulty is concurrency: because threads are executed concurrently possibly on different processors, and are subject to operating system scheduling decisions, interrupts, etc., we must think of the interaction between threads as completely asynchronous, so that the steps of different threads can be interleaved arbitrarily. %This significantly complicates the task of designing correct concurrent data structures. 

%
%
%There are several techniques to construct concurrent data structures including coarse-grained locking, fine-grained locking, and lock-free programming. The simplest technique is coarse-grained locking, where, a single lock is used to synchronize every access to an object. Coarse-grained locking is easy to reason about, however it works well only when the level of concurrency is low. However if too many threads try to access an object at the same time, then the object becomes a sequential bottleneck, forcing threads to wait in line for access. Therefore, “Fine-grained synchronization techniques address this problem by splitting the object into independently synchronized components, ensuring that method calls interfere only when trying to access the same component at the same time. Fine-grained locking requires very careful design of the data structure and its methods, since one must foresee what can happen when several threads access the same component in parallel.
%Fine-grained synchronization
%is often performed without locks, replacing them by less costly
%synchronization operations such as lock-freee technique. The lock-free technique help us to eliminate locks entirely, it relies on built-in atomic operations such as {\tt compareAndSet()} for synchronization. Each of these techniques can be applied (with appropriate customization) to a variety of common data structures (queues, stacks, sets) implemented by different linked data structures such as singly linked lists, skiplists, trees, or lists of lists. 
%


%Processes are sequential, each process applies a sequence of its operations to a share structures. A process can execute at a varying rate. Actually, we do not tell whether a process is halted or is running fast or slowly. Each object has a sequential specification that defines how the object behaves when its operations are
%invoked one at a time by a single process. An object’s operations can be invoked by concurrent
%processes, and it is necessary to give a meaning to interleaved operation
%executions.
%An object is linearizable if each operation appears
%to take effect instantaneously at some point between the operation’s invocation
%and response. Linearizability implies that processes appear to be interleaved
%at the granularity of complete operations, and that the order of
%nonoverlapping operations is preserved. The notion of linearizability generalizes and uniles a number of ad hoc correctness conditions in the literature, and it is related to (but not identical with) correctness criteria such as sequential consistency and strict serializability.

A concurrent data structure is a way of storing and organizing data for access and manipulation by multiple computing threads (or processes) on a shared-memory computer. Each operation is implemented as a sequential
method that is executed by a thread. Several features of shared-memory multiprocessors make concurrent data structures significantly more difficult to design and to verify as correct than their sequential counterparts. The primary source of this additional difficulty is concurrency: because threads are executed concurrently possibly on different processors, and are subject to operating system scheduling decisions, interrupts, etc., we must think of the interaction between threads as completely asynchronous, so that the steps of different threads can be interleaved arbitrarily. %This significantly complicates the task of designing correct concurrent data structures. 



There are several techniques to construct concurrent data structures including coarse-grained locking, fine-grained locking, and lock-free programming. The simplest technique is coarse-grained locking, where a single lock is used to synchronize every access to an object. Coarse-grained locking is easy to reason about, however it works well only when the level of concurrency is low. However, if too many threads try to access an object at the same time, then the object becomes a sequential bottleneck, forcing threads to wait in line for access. Therefore, Fine-grained synchronization techniques address this problem by splitting the object into independently synchronized components, ensuring that method calls interfere only when trying to access the same component at the same time. Fine-grained locking requires very careful design of the data structure and its
methods, since one must foresee what can happen when several threads access
the same component in parallel.
%There is a trick in this technique called lazy synchronization.  The task of removing a component from a data structure can be split into two phases: the component is logically removed simply by setting a tag bit, and later, the component can be physically removed by unlinking it from the rest of the data structure. 
Fine-grained synchronization is often performed without locks, replacing them by less costly
synchronization operations such as {\tt compareAndSet()}. Each of these techniques can be applied (with appropriate customization) to a variety of common data structures (queues, stacks, sets) implemented by different linked data structures such as singly linked lists, skiplists, trees, or lists of lists. 




%_____________User for linearization section
%Processes are sequential, each process applies a sequence of its operations to a share structures. A process can execute at a varying rate. Actually, we do not tell whether a process is halted or is running fast or slowly. Each object has a sequential specification that defines how the object behaves when its operations are
%invoked one at a time by a single process. An object’s operations can be invoked by concurrent
%processes, and it is necessary to give a meaning to interleaved operation
%executions.
%An object is linearizable if each operation appears
%to take effect instantaneously at some point between the operation’s invocation
%and response. Linearizability implies that processes appear to be interleaved
%at the granularity of complete operations, and that the order of
%nonoverlapping operations is preserved. The notion of linearizability generalizes and uniles a number of ad hoc correctness conditions in the literature, and it is related to (but not identical with) correctness criteria such as sequential consistency and strict serializability.
%>>>>>>> 5430563f072f0eab8df6b28b9ef7a63119d17862
 
  
%We consider systems consisting of an arbitrary number of concurrently executing threads. Each thread may at any time invoke one of a finite set of methods. Each method corresponding to one operation on the data structure.
%Each method declares local variables and a method body.
%We assume that the local variables include the
%program counter $\tt pc$ and also potentially
%include an input parameter of the method.
%%
%The body is built in the standard way
%from atomic commands using standard control
%flow constructs (sequential composition, selection, and loop constructs).
%%
%Each run of the program consists of an arbitrary (but finite) number of
%concurrently executing threads. 
%%
%Each thread invokes one of the methods.
%%
%Thread execution is terminated by executing a {\tt return} command,
%which may return a value.
%%
%The shared variables can be
%accessed by all threads, whereas local variables can be accessed only
%by the thread which is invoking the corresponding method.
%%
%We assume that the global variables and the heap are initialized by
%an initialization thread, which is executed once at the beginning
%of program execution.
%%
%Furthermore, we assume that the local variables  have 
%arbitrary initial values.
%%
%In this thesis, we assume that variables are either pointer variables
%(to heap cells) or data variables.
%%
%The data variables assume values 
%from an unbounded or infinite (ordered) domain,
%or from some finite set $\mathbb F$.
%%
%We assume w.l.o.g. that the infinite set is given by the set $\mathbb Z$ of integers.
%
%%
%%As usual, we will also use arbitrary finite domains that we built from $\boolset$.
%%
%A parameter of a method may be instantiated by any value in $\mathbb Z$.
%%
%Heap cells can have a number of data fields that contain data either from
%$\mathbb Z$ or $\mathbb F$.
%%
%A cell has only one pointer field, denoted {\tt next}.
%%
%Atomic commands include assignments between data variables, 
%pointer variables, or fields of cells pointed to by a pointer variable.
%%
%The command {\tt new Node()} allocates a new structure of type
%{\tt Node} on the heap, and returns a reference to it.
%%
%The compare-and-swap command {\tt CAS(\&a,b,c)} atomically
%compares the values of {\tt a} and {\tt b}.
%If  equal, it assigns the value of
%{\tt c} to {\tt a}  and returns {\tt true}, 
%otherwise, it leaves {\tt a} unchanged and returns {\tt false}. 
%%% 
%We assume that each statement in  a method
%has a unique label.
%
%
%
%%\endgroup
%
%
%\input heap-lazy-list
\vspace{1cm}
\input img/lazy-list



%
%
%Each method declares local variables (including the input parameters
%of the method) and a method body.
%%
%In this chapter, we assume that variables are either pointer variables
%(to heap cells), or data variables (assuming values from an unbounded
%or infinite domain%, which will be denoted
%~$\dset$).
%%
%The body is built in the standard way from atomic commands using
%standard control flow constructs (sequential composition, branching,
%and loop constructs).
%%
%Method execution is terminated by executing a \prgcode{return}
%command, which may return a value.
%%
%The global variables can be accessed by all threads, whereas local
%variables can be accessed only by the thread which is invoking the
%corresponding method.
%%
%We assume that the global variables and the heap are initialized by an
%initialization method, which is executed once at the beginning of
%program execution.
%
%Programs manipulate heap cells of type \prgcode{node}, each consisting
%of a~\prgcode{val} field and a~\prgcode{next} field, which carry
%respectively a data value and the address to another heap cell.
%%
%Atomic commands include assignments between data variables, pointer
%variables, or fields of cells pointed to by a~pointer variable.
%%
%The command \prgcode{new node()} allocates a new structure of
%type \prgcode{node} on the heap, and returns a reference to it.
%%
%\index{Compare-and-Swap (CAS)}%
%The compare-and-swap command \prgcode{CAS(a,b,c)} compares the memory
%locations \prgcode{a} and \prgcode{b}. If equal, it also atomically
%assigns the value~\prgcode{c} to \prgcode{a} and returns
%\prgcode{TRUE}. Otherwise, it leaves \prgcode{a} unchanged and returns
%\prgcode{FALSE}.
%%
%Note that \prgcode{a}, \prgcode{b} and \prgcode{c} can be pointers or
%variables using the referencing and dereferencing C constructs
%\prgcode{\&}~and~\prgcode{*}.

%\endgroup

As an example, Fig.~\ref{figure:lazy-list} depicts a program
{\tt Lazy Set} \cite{Lazyset}
that implements a concurrent set containing integer
elements with three operations $\tt add$, $\tt remove$ and $\tt contains$.
%<<<<<<< HEAD
%%
%The set is constructed by using lazy synchronization technique (small trick of fine-grained synchronization) in which the task of removing a component from a data structure can be split into two phases: the component is logically removed simply by setting a tag bit, and later, the component can be physically removed by unlinking it from the rest of the data structure. 
% and  implemented as an ordered singly linked list. A cell in the list has three fields ${\tt mark}$, ${\tt lock}$, and
%${\tt val}$. The field $\tt mark$ is true if
%the node has been logically removed from the set. The ${\tt lock}$ field is a lock and the field ${\tt val}$ presents the data value which is integer in this case.
%The program contains three methods, namely {\tt add}, {\tt rmv},
%and {\tt ctn},  corresponding to operations $\tt add$, $\tt remove$ and $\tt contains$
%that respectively add, remove, and check the existence
%of an element in the set. Each method takes an argument which is the value of the element, and returns a value which indicates whether
%the operation has been successful or not. For instance, the operation {\tt add} returns the value
%{\it true} if  $\tt e$ is not already a member of the set. If $\tt e$ is already present, then the list is not changed and the value {\tt false} is returned.
%=======
It is just as the sequential version,
but that each cell now has two additional fields fields ${\tt mark}$, ${\tt lock}$. The field $\tt mark$ is \true\; if
the node has been logically removed from the set. The ${\tt lock}$ field is a lock and the field ${\tt val}$ presents the data value which is integer in this case. The
mechanism behind logically and physical removing is explained as following: it is impossible to atomically remove a cell from the list if other threads may concurrently access the adjacent cells. One reason is that one must both move a $\tt next$ pointer which reference to the cell and physically remove the cell. This cannot be done, e.g., if another thread currently is visiting the cell that is to be removed. Therefore, the task of removing a cell from the list  can be split into two phases: the cell is logically removed simply by setting a $mark$ field to be \true, and later, the cell can be physically deleted by unlinking it from the rest of the data structure. The removal “actually happens” when an entry is marked, and the physical removal is just a way to clean up
%The set is constructed by using lazy synchronization technique (small trick of fine-grained synchronization) in which the task of removing a component from a data structure can be split into two phases: the component is logically removed simply by setting a tag bit, and later, the component can be physically removed by unlinking it from the rest of the data structure. 
 %and  implemented as an ordered singly linked list. A cell in the list has three fields ${\tt mark}$, ${\tt lock}$, and
%${\tt val}$. The field $\tt mark$ is true if
%the node has been logically removed from the set. The ${\tt lock}$ field is a lock and the field ${\tt val}$ presents the data value which is integer in this case.
%The program contains three methods, namely {\tt add}, {\tt rmv},
%and {\tt ctn},  corresponding to operations $\tt add$, $\tt remove$ and $\tt contains$
%that respectively add, remove, and check the existence
%of an element in the set. Each method takes an argument which is the value of the element, and returns a value which indicates whether
%the operation has been successful or not. For instance, the operation {\tt add} returns the value
%{\it true} if  $\tt e$ is not already a member of the set. If $\tt e$ is already present, then the list is not changed and the value {\tt false} is returned.
%>>>>>>> 5430563f072f0eab8df6b28b9ef7a63119d17862
% 
The algorithm uses two global pointers, {\tt head} that points to  the first cell of the heap, and {\tt tail} that points to the last cell.  
These two cells contain two values that are smaller 
and larger respectively than data values of all cells that may be                     
inserted in the set. The algorithm also contains the subroutine {\tt locate} that returns a structure containing the cells on either side of $\tt e$. In more detail, the {\tt locate} method traverses the list using two local variables ${\tt p}$ and $\tt c$, starting at the head of the list and moves forward the list (line 2), comparing ${\tt c.val}$ to ${\tt e}$. When $\tt c$ is set to the
first cell whose the value of  $\tt val$ is greater than or equal to $\tt e$, the traversal stops, and the
method locks cells pointed to by $\tt p$ and $\tt c$ (line 7) so that no other thread can update fields of $\tt p$ and $\tt c$. Thereafter, if both $\tt p.mark$ and $\tt c.mark$ are \false \; and ${\tt p.next = c}$ meaning that there is no added cell from other thread between $\tt p$ and $\tt c$ then the method returns the pair $\tt (p,c)$ (line 9). Otherwise, it unlocks cells pointed to by $\tt p$ and $\tt c$ and tries traversing again from the head of the list.


%The {\tt rmv} method first logically removes the node
%from the list by setting the {\tt mark} field, before 
%physically removing the node.
%
%The {\tt ctn} method traverses the list ignoring the locks
%inside the cells. 

 


The {\tt add(e)} method calls {\tt locate(e)} at line 1 to locate the position in the list where $\tt e$ is to be inserted. Its local variables $\tt p$ and $\tt c$ are assigned the first and second values of the pair return by {\tt locate(e)} respectively. If $\tt c.val = e$ meaning that a cell whose data value of $\tt val$ is equal to $\tt e$ is already in the list, the method unlocks $\tt p$ and $\tt c$ and returns \false \; (line 7-8). Otherwise, a new cell $\tt n$ is created (line 3), and inserted into the list by linking it into the list between the $\tt p$ and $\tt c$ pointers returned by
{\tt locate} (line 3-4). Then, the method unlocks cells pointed to by $\tt p$ and $\tt c$ and returns \true.  

The {\tt rmv(e)} method also calls {\tt locate} at line 1 locate the position in the list where $\tt e$ is to be inserted. If $\tt c.val \neq e$ meaning that a cell with val $\tt e$ is not already in the list, the method unlocks $\tt p$ and $\tt c$ and returns \false (line 7-9). Otherwise, cell $\tt c$ is logically removed (line 3) where the $\tt mark$ field of $\tt c$ gets assigned \true, and unlinked from the list (line 4-5). Then, the method unlocks cells pointed to by $\tt p$ and $\tt c$ and returns \true.  

The {\tt ctn(e)} method traverses the list by using local variable $\tt c$, ignoring whether nodes are marked or not, until $\tt c$ is set to the
first cell with a value of $\tt val$ greater than or equal to $\tt e$. It simply returns \true \; if and only if the cell pointed to by $\tt c$  is unmarked with the desired value of $\tt val$ equal to $\tt e$. 
%Come later
%The program is linearizable if it respects the sequential specification of a set. Informally, for each method {\tt add}, {\tt rmv} and {\tt ctn}, there exist a linearization point so that in each execution of the program, when these methods are ordered according to their linearization points. We get a correct sequence that respect the sequential specification of a set.  In this program, linearization point of unsuccessful {\tt ctn} is not stayed at the code of the method. Whereas, other linearization points of other methods stay at their codes. We explain more detail about this in the linearization policies section. 
%\begin{figure}[]
%  \begin{tikzpicture}
%  %[
%   % property/.append style={left,rotate=-10,scale=0.7,shift={(-0.5,-0.7)}},
%   % ]
%    \node[codeblock] (struct) {\begingroup\scriptsize\VerbatimInput[numbers=none]{experiments/code/treiber/struct-gc}\endgroup};
%   % \node[codeblock] (init) at (struct.north east) {\begingroup\scriptsize\VerbatimInput[numbers=none]{experiments/code/treiber/init-gc}\endgroup};
%    
%
%    
%    \node[codeblock,below right] (push) at (struct.south west) {\begingroup\scriptsize\VerbatimInput{experiments/code/treiber/push-gc}\endgroup};%locate
%    
%        \node[codeblock,below right] (pop) at (struct.south east) {\begingroup\scriptsize\VerbatimInput[firstnumber=10]{experiments/code/treiber/pop-gc}\endgroup};
%        
%            \node[codeblock, below] (cnt) at (push.south) {\begingroup\scriptsize\VerbatimInput[firstnumber=10]{experiments/code/treiber/cnt}\endgroup};
%    
%                \node[codeblock, below] (rmv) at (pop.south) {\begingroup\scriptsize\VerbatimInput[firstnumber=10]{experiments/code/treiber/rmv}\endgroup};
% %   \node[property] at (init.north east) {\sc Init};
%  %  \node[property] at (push.north east) {\sc Push};
%  %  \node[property] at (pop.north east) {\sc Pop};
%    
%  \end{tikzpicture}
%  \caption{Lazy set.}
%  \label{figure:lazy-list}
%\end{figure}

%\begingroup%
%\setlength\intextsep{\dazintextsep}
%\begin{figure}[ht]
%  \centering
%  \tikzinput{img/shape-heap}
%  \caption{Memory layout of a Treiber stack (with $\dset=\nat$),
%    showing only two threads.}% out of all
%  \label{figure:shape:heap}
%\end{figure}
%\endgroup
