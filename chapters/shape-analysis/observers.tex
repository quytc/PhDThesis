%% ====================================================================
%\chapter{Verification of Linearizabilities}
\chapter{Specifying Linearizability}
In the previous sections, we describe the correctness criterion of linearizability for concurrent data structures. The goal is to specify linearizability in a way that is suitable for automated verification.
%Firstly, we explain the definition of \emph{observer} which is used to specify sequential specification of data structures. Thereafter, we describe how to verify linearization properties \bjcom{strange term} by using \bjcom{You should rather speak about ``linearization policies''}
%our \emph{controller} which is used to specify how a data structure is linearizable.
We separate the problem of specifying linearizability into several ones.
\begin{itemize}

 \item  To specify the sequential semantics of a data structure in a way that is suitable for automated verification.
  \item To specify placement of linearization points in executions of a concurrent data structure.
  	 
\end{itemize}
  For the first, we use the techniques of observers \cite{AHHR:integrated:rep}. For the second, we present a new technique, in which methods are equipped with controllers. Controllers specify so-called ``linearization policies'', which prescribe how LPs are placed in executions.
  %You can also add a section, where you survey the technique of observers that synchronize at call and return events
 

\section*{Observers}
\label{section:observers}
\index{Observers}
%% ====================================================================
%\bjcom{This introduction does not work. It is not an introduction to observers, but rather says how they are used in linearization policies. Some of the
%material can be moved to the survey section before the ``Observers'' section (see above)}
%
%In order to derive the totally ordered execution from a concurrent
%execution, each method is instrumented to generate a so-called
%abstract event whenever a linearization point is passed. Right after the program pass the linearization point, the abstract event is
%communicated to an external \emph{observer}, which records the sequences of
%abstract events from the code execution. In the next paragraph, we introduce the notion of observer, which essentially separates good traces of events from bad ones. Several observers shall be used to specify the safety property
Data structures are, by nature, infinite-state objects, since they are intended to carry an unbounded number of data elements. For automated verification, it is desirable with specifications that are constructed without explicitly mentioning such infinite objects.  This problem is addressed by observers \cite{AHHR:integrated:rep}. Observers specify allowed sequences of operations by constraining their projection on a small number of data elements.
% For instance, a stack data structure is specified by (* continue here *)
%We specify the sequencial \bjcom{please spell check!!!!} semantics of data structures by observers, as introduced
%in \cite{AHHR:integrated:rep}.
Observers are finite automata extended with a finite set of observer \emph{registers} that
assume values in integer domain. At initialization, the registers are nondeterministically assigned arbitrary
values, which never change during a run of the observer. 
\vspace{1cm}
\begin{figure}[h]
%\begin{wrapfigure}{r}{0.5\textwidth} 
  \centering
  \tikzinput{img/set-observer} 
  \vspace{0.3cm}
  \caption{A stack observer}
  \label{figure:shape:set:observers}
%\end{wrapfigure}
\end{figure} \vspace{1cm} Transitions are labeled
by operations that may be parameterized on registers. Observers are used as
acceptors of sequences of operations. The observer processes such sequences
one operation at a time. If there is a transition, whose label, after replacing registers by their values, matches the operation, such a transition is performed. If there is no such transition,
the observer remains in its current state. The observer accepts a sequence if it can be
processed in such a way that an accepting state is reached. We use observers to give exact specifications of the behaviors of data structures such as sets, queues, and stacks. The observer is defined in such a way that it accepts
precisely those sequences of abstract operations that are not allowed by the semantics of
the data structure. This is best illustrated by an example. Fig. 3 depicts an observer that
accepts the set of method invocations that are not allowed by sequential specification of a set. The observer have three states $s_0$, $s_1$ and $s_2$. The initial state $s_0$ corresponds to positions in the runs where the non-determnistically tracked value
stored in the observer variable $z$ is not present in the set (i.e.
each time it has been inserted it got deleted afterwards). The
state $s_1$ corresponds to positions in the runs where the tracked
value is present in the set (i.e. it has not been deleted since it
was last inserted). The accepting state $s_2$ corresponds to positions
in the runs where the bad specification captured by the observer has been observed. The captured bad specification are those where
a data value is deleted or found although it is not present in the set, or a data value is not found or cannot be deleted although it is already present in the set
%\bjcom{Continue and explain in 5-10 lines how it works}

\section*{Linearization Policies}
\label{controllers:subsection}
In order to prove linearizability, the most intuitive approach is to find a linearization point (LP) in the code of the implementation, and show that it is the single point where the effect of the operation takes place \cite{AHHR:integrated:rep,BLMRS:cav08,Vafeiadis}.
\input lazy-list-rrules
%\bjcom{The preceding text in this paragraph can be moved and adapted to the earlier paragraphs of this chapter, where you explaine fixed and non-fixed LPs}
However, for a large class of linearizable implementations,
it is not possible to assign fixed LPs in the code of their methods, 
but depend on actions of other threads in each particular execution. For example, 
%\bjcom{A problem here is that the lazy set does not use ``helping''. PLease adapt the text to avoid such a misunderstanding}
in the {\tt Lazy Set} algorithm, a successful {\tt rmv(e)} method has its LP at line 3, and an unsuccessful {\tt rmv(e)} has its LP at line 2 when the test $\tt c.val = e$ evaluates to \false. A successful {\tt add(e)} method has its LP at line 4 and an unsuccessful {\tt add} has its LP at line 2 when the test $\tt c.val <> e$ evaluates to \false\;. The successful {\tt ctn} is linearized at line 4 then the value of $\tt b$ is \true. 
However it is not possible to assign a fixed LP in the code of the {\tt ctn} method.

To see why unsuccessful {\tt ctn} method invocations may not have fixed LPs, notethat the naive attempt of defining the LP at line 4 provided that the test at line 5 failswill not work. Namely, the {\tt ctn} method may traverse the list and arrive at line 4 in asituation where the element e is not in the list (either e is not in any cell, or the cellcontaining $\tt e$ is marked). However, before executing the command at line 4, anotherthread performs an add operation, inserting a new cell containing the element $\tt e$ intothe list. The problem is now that the {\tt ctn} method cannot “see” the new cell, since it isunreachable from the cell currently pointed to by the variable b of the {\tt ctn} method. Ifthe {\tt ctn} method would now try to linearize an unsuccessful {\tt ctn}, this would violatethe semantics of a set, since the add method just linearized a successful insertion of $\tt e$.
%However, linearizing an unsuccessful {\tt ctn(e)} is a bit tricky, and is a good example showing that it is not always
%possible to define a single linearization point for each method that works for all
%methods in all executions. In particular, simply choosing the linearization
%point for an unsuccessful {\tt ctn(e)} as the point at which a cell whose $\tt mark$ field is \true\; or a cell whose value of $\tt val$ is greater than the $\tt e$ (line 5) is found is
%incorrect. Consider the following case. Assume that a marked cell $\tt a$ where $\tt a.val = e$ is found and
%thread A is attempting to execute the method {\tt ctn(e)}. While A is traversing
%the list, $\tt c$ and all cells between $\tt c$ and $\tt a$ both are
%logically and physically removed. Thread A would still proceed until the point where $\tt c$ points to $\tt a$. Then, It detects that $\tt a$ is marked and therefore no longer in the list. It is correct to put its LP at this point in this case. However, consider what
%happens if while thread A is traversing the removed section of the list which leads to $\tt a$, another thread adds a new cell whose value of $\tt val$ is equal to $\tt e$ into the list. It is wrong to put LP at the point when $\tt c$ points to $\tt a$,
%since it occurs after the insertion of the new cell with same value of $\tt val$ a to the list. The method {\tt ctn(e)} is linearized at the earlier of the following points: (I) the point where a
%removed matching cell is found and (II) the point immediately before a new
%matching cell is added to the list. As can be seen, this linearization.
%
%\bjcom{The previous sentence is sloppy. You probably mean that ``it is not possible to assign a fixed LP in the code of the ctn method''. Your current text
%  may give the impression that there is an LP, but it is not fixed: it is unclear what this means}
%It stays in the code of the {\tt add} method.
%\bjcom{I suspect that the previous statement is incorrect. As I understand it, there is no fixed LP for ctn in the add method, since the placement depends on
%  what happens in the execution}
%\bjcom{Here, you must insert a detailed explanation of why this is the case, and how linearization points can be assigned in each execution. This will easily be
%10-20 lines. After that, end the paragraph}

 There have been several previous works dealing with the problems of non-fixed linearization points \cite{Poling,Colvin:Lazy-List,CGLM:cav06,SWD:cav12,Derrick:fm14,SDW:tcl14,Vafeiadis:cav10,Vafeiadis:Aspect}. However, they are either manual approaches without tool implementation or not strong general enough to cover various types of concurrent programs. 
 %\bjcom{You must comment in more detail about these works in a ``related works'' section of the intro}
 In this thesis we handle non-fixed linearization points by providing semantic for specifying linearization policies by a mechanism for assigning LPs to executions, which we call \emph{linearization policies}. %The linearization point of a thread may be defined in two ways: \bjcom{Again, do not say that there are EXACTLY two ways}
%(i) The thread may define its own linearization point,
%and in that case may also help other threads define their 
%own linearization points.
%(ii) The thread may be helped by other threads.
%\bjcom{The points (i) and (ii) are almost impossible to understand, since you do not explain anything about what happens}
%\bjcom{I suggest you replace the preceding (i) and (ii) by giving a couple of EXAMPLES of how methods may linearize. You already have explained on
%  in the preceding paragraph (for lazy set). You can informally present a helping mechanism, which you can borrow from some other algorithm that
%  you verify. In all cases, you must explain concretely what happens in the concurrent algorithm, not just be abstract}
%The helping mechanism may contain complicated patterns.
%%

A linearization policy is expressed by defining for each method an associated controller, which is responsible for generating operations announcing the occurrence of LPs during each method invocation. The controller is occasionally activated, either by its thread or by another controller, and mediates the interaction of the thread with theobserver as well as with other threads.

To add controllers, we first declare 
some statements in each method to be {\it triggering}: 
these are marked by the symbol \commitpoint,
as in Figure~\ref{lazy:list:fig}.
%
We specify the behavior of the controller, belonging to a method
$\mname$, by a set  of
{\em reaction rules}. 
%
%
To define these rules, we first define different types of events
that are used to specify their behaviors.
%
Recall that an {\it operation} is of the form $\mname(\indata,\outdata)$ where 
$\mname$ is a method name and $\indata$, $\outdata$ are data values.
Operations are emitted by the controller to the observer to notify 
that the thread executing the method performs a linearization of
the corresponding method with the given input and output values.
Next, we fix a set $\msgset$ of {\it broadcast messages},
each with a fixed  arity, which are used for
synchronization between controllers. A message is formed by supplying data
values as parameters. In reaction rules, these data values are denoted by
expressions over the variables of the method, which are evaluated in the
current state when the rule is invoked. In an operation, the first
parameter, denoting input, must be either a constant
or the parameter of the method call.

%% The set will be used to send and receive boradcast messages
%% to/from other threads.
%% %
%% A {\it send event} $\sevent$ is of the form
%% $\msg(\expr_1,\ldots,\expr_\nn)$ where 
%% $\msg\in\msgset$ is a message of arity $\nn$,
%% $\expr_1,\ldots,\expr_\nn$ are either 
%% local variables or constants.
%% %
%% %
%% We define $\msgof\sevent:=\msg$.
%% %
%% A {\it receive event} $\revent$ is of the form
%% $\msg(\xvar_1,\ldots,\xvar_\nn)$
%% where 
%% $\msg\in\msgset$ is a message with arity $\nn$, and
%% $\set{\xvar_1,\ldots,\xvar_\nn}$ is a set of variables (parameters)
%% that is joint from the set of local varibales of the thread.
%% %
%% We define $\msgof\revent:=\msg$.
%% %
%% An event (observer send or rceive) is said to be {\it ground} 
%% if all its arguments are constants.

\begin{itemize}
\item
  A {\em triggered} rule, of form
\(
%% \sndrule\cstate{\triggersym}\cnd\oevent\sevent{\cstate'}
\sndrulenostate{\triggersym}\cnd\oevent\sevent
\), 
specifies that whenever the method executes a triggering statement
and the condition $\cnd$ evaluates to {\tt true}, then 
the controller performs a {\em reaction} in which it emits the operation
obtained by evaluating $\oevent$
to the observer, and broadcasts the message obtained by evaluating $\sevent$
to the controllers of other threads.
%
%% Here $\cnd$ is a predicate on the set of local variables of the thread.
%
The broadcast message $\sevent$ is optional.
%\todo{We do not mention the object if it is not there.}
\item
  A {\em receiving} rule, of form
\(
\rcvrulenostate{\tuple{\revent,\ord}}\cnd\oevent
\),
specifies that whenever the observer of some other thread broadcasts
the message obtained by evaluating $\revent$, 
and $\cnd$ evaluates to {\tt true},
then the controller performs a
reaction where it emits the operation obtained by evaluating
$\oevent$ to the observer.
%
Note that no further broadcasting is
performed.
%
The interaction of the thread with the observer 
may occur either
{\it before} or {\it after}
the sender thread, according to the flag $\ord$.
\end{itemize}
%% In these rules, conditions $\cnd$, operations $\oevent$, and messages
%% $\sevent$, $\revent$ are expressions over the state accessible to the method;
%% they are evaluated in the current state when the rule is invoked.
A controller may also use a finite set of states,  which restrict the
possible sequences of reactions by a controller in the standard way.
Whenever such states are used, the rule includes source and target states
using kewords {\bf from} and {\bf goto}.
In Figure~\ref{rrules:lazy:list:fig}, the rule $\rulename_7$
changes the state from $q_0$ to $q_1$, meaning that no further applications
of rules $\rulename_6$ or $\rulename_7$ are possible, since they both
start from state $q_0$. Rules that do not mention states can be
applied regardless of the controller state and leave it unchanged.

%Let us  illustrate how the reaction rules for controllers in
%Figure~\ref{rrules:lazy:list:fig} specify LPs
%for the algorithm in Figure~\ref{lazy:list:fig}.
%Here, a successful {\tt rmv} method has its LP at
%line {\tt 3}, and
%an unsuccessful {\tt rmv} has its LP at line {\tt 2} when the test
%{\tt c.val = e} evaluates to {\tt false}.
%Therefore, both these statements are marked as triggering.
%The controller has a reaction rule for each of these cases: in
%Figure~\ref{rrules:lazy:list:fig}:
%rule $\rulename_3$ corresponds to a successful {\tt rmv},
%whereas
%rule $\rulename_4$ corresponds to an unsuccessful {\tt rmv}.
%Rule $\rulename_4$ states that whenever the {\tt rmv}
%method executes a
%triggering statement, from a state where {\tt pc=2} and {\tt c.val <> e}, then
%the operation {\tt rmv(e,false)} will be emitted to the observer.
%
%A successful {\tt add} method has its LP at line {\tt 4}. Therefore, the
%controller for {\tt add} has the triggered rule $\rulename_1$ which emits
%the operation {\tt add(e,true)} to the observer. In addition, the
%controller
%also broadcasts the message {\tt add(e)}, which is
%received by any controller for a {\tt ctn} method which has not yet
%passed line {\tt 4}, thereby
%linearizing an unsuccessful {\tt ctn(e)} method by
%emitting {\tt ctn(e,false)} to the observer.
%  The keyword {\bf before} denotes that the operation
%  {\tt ctn(e,false)} will be presented before {\tt add(e,true)} to the observer.
%Since the reception of {\tt add(e)} is performed in the same atomic step as the
%triggering statement at line {\tt 4} of the {\tt add} method, this
%describes a linearization pattern, where
%a {\tt ctn} method, which has not yet reached line {\tt 4}, linearizes an
%unsuccessful {\tt ctn}-invocation just before some other thread linearizes
%a successful {\tt add} of the same element. 
%
%To see why unsuccessful {\tt ctn} method invocations may not have fixed LPs,
%note that the naive attempt of defining the LP at line {\tt 4}
%provided  that the test at line {\tt 5} fails will not work. Namely, the
%{\tt ctn} method may traverse the list and arrive at line {\tt 4} in
%a situation where the element {\tt e} is not in the list (either {\tt e} is
%not in any cell, or the cell containing {\tt e} is marked). However, before
%executing the command at line {\tt 4}, another thread performs an {\tt add}
%operation, inserting a new cell containing the element {\tt e} into the list.
%The problem is now that
%the {\tt ctn} method cannot ``see'' the new cell, since it is unreachable from
%the cell currently pointed to by the variable {\tt b} of the
%{\tt ctn} method. If the {\tt ctn}
%method would now try to linearize an unsuccessful {\tt ctn(e)}, this would
%violate the semantics of a set, since the {\tt add} method just linearized
%a successful insertion of {\tt e}.
%
%A solution to this problem, following~\cite{Lazyset}, is to let an
%unsuccessful {\tt ctn(e)} method linearize at line {\tt 4} {\em only if}
%no successful {\tt add(e)} method linearized since its invocation. If
%some other thread linearizes a successful {\tt add(e)} before the {\tt ctn(e)}
%method executes line {\tt 4}, then the {\tt ctn(e)} method should linearize
%immediately before the {\tt add(e)} method.
%This solution is expressed by the rules in Figure~\ref{rrules:lazy:list:fig}.
%Note,
%however, that it is now possible for an invocation of a successful
%{\tt ctn(e)} to emit several operations: It can
%first emit {\tt ctn(e,false)} together with the linearization of a
%successful {\tt add(e)}, and thereafter emit
%{\tt ctn(e,true)} when it executes the statement at line 4, if it then
%finds the element {\tt e} in the list (by rule $\rulename_5$).
%In such a case, both operations are fine with the observer,
%and the last one is the event that causes the method to return {\tt true}, i.e.,
%conforms with the parameters and returns values.
%
%%As can be seen from the example, a method can possibly generate several linearization points. An obvious solutions is the use of speculation 
%%idea and develop a forward-backward simulation. We keep both speculations after the potential linearization points, Then at the validation step we commit to the correct branch. However, this approach would be inefficient in practice. Therefore in this work we reduce verifying linearizability to control-state reachability. Instead of ensuring linearizability directly, we verify the condition which is stronger than linearizability as below
%%\todo{Improve this.}
%%Thus, adding observer and controllers reduces verification
%%to establishing that
%%\todo{Improve this.}
%%\begin{enumerate}[(i)]
%%\item
%%  each method invocation generates a non-empty sequence of operations,
%%\item
%%  the last operation of a method conforms to its parameters and return value,
%%\item
%%  only the last operation of a method may
%%  change the state of the observer,
%%\item
%%  the sequence of all operations
%%  cannot drive the observer to an accepting state.
%%\end{enumerate}
%

%
%\bjcom{The preceding paragraph has the fundamental problem that you do not describe how the ALGORITHM works, but
%  rather you describe what the CONTROLLERS do. The controllers are not part of the algorithm (they will be introduced only
%  later, when you describe how they work}
%\bjcom{To be more precise on ``helping''. My understanding is that in concurrent algorithms, ``helping'' means that one method
%  actively helps another method to perform an operation, e.g., to remove an element. In your above text, you use ``helping''
%  in another sense: namly that the LP of one method can be placed in the operation of another. This is different, and has nothing
%  to do with the working of the algorithm, it is only relevant for reasoning about linearizability. So, if you discuss ``helping'', it
%  should be in an example where the methods actually help each other}
%
%
\input img/policy 

%To specify the linearization patterns \bjcom{new undefined term, you can use ``various ways in which LPs can be assigned}, we equip each method with a
%{\it controller} whose behavior is defined by a set of rules which are described detail in paper II.
%%
%\bjcom{You need a couple of more sentences here to explain what is the role of controllers}
%The controller is occasionally activated by the thread,
%and helps organize the interaction  
%of the thread with other threads as well as with the observer.
%%
%More precisely, some 
%statements in a  method are declared to be {\it triggering}. If a triggering statement is executed then 
%the controller of the thread will also be executed simultaneously.  
%

%\bjcom{The rest of this paragraph fits into the text as it is}
 Let us describe an example of how the {\tt controller} handles non-fixed linearization point of {\tt ctn} method. Figure \ref{fig:policy} give an example of {\tt Lazy Set} with three threads $\tt 1$, $\tt 2$, and $\tt 3$. The thread $\tt 1$ is executing the {\tt add} to add the element {\tt e} into the set, whereas $\tt 2$ and $\tt 3$ are executing {\tt ctn} to lock for the element {\tt e} in the set. When the thread $\tt 1$ reaches the triggering statement at line 4 of the {\tt add} method at the step $\tt 1$. The controller rule 
%the linearization point of the {\tt add} operation
%is defined statically in the code of the method
%(lines {\tt 2} and {\tt 4}).
%%
%There are two possible linearization points
%corresponding to whether the 
%operation is {\it unsuccessful} (the element to be added is already in the 
%list), or {\it successful} (the element is not in the list.)
%%
%In the first case, the thread reaches the triggering statement
%at line {\tt 2} and the condition ``{\tt c.val = e}''  holds,
%i.e., we have found the element {\tt e} in the list. Thereafter, it informs the observer that an {\tt add} operation 
%with argument {\tt e} has been performed, and that the outcome
%of the operation is {\tt false} (the operation was unsuccessful.)
%%
Before a successful {\tt add} operation is communicated to the observer 
to informs the observer that an {\tt add} operation 
with argument {\tt e} has been performed, and that the outcome
of the operation is {\tt true} (the operation was successful) in the step $\tt 6$. The controller will help other threads
by to linearize. This is done by {\it broadcasting} a message to the threads $\tt 2$, $\tt 3$ which is executing {\tt ctn} in steps $\tt 2$, $\tt 3$. These threads then inform the observer that the element e is not in the list in $\tt 3$, $\tt 4$ respectively. Note that $\tt 2$, $\tt 3$ can get message in any order. The detail of controller is described in paper II.
%\bjcom{The description of linearization policy in the preceding paragraph is in principle OK. But the grammar and language is not acceptable. There are even unfinished sentences. Please make an effort to write nice english, explaining in a way that colleagues can easily understand how things work.}
%
%\bjcom{Write more about linearization policies. You should by example explain syntax of controllers, intuitively what they can do, etc.
%  For instance, how they reduce checking for linearizability to checking (non)reachability of bad states. This includes all the stuff that is
%currently done by monitors.
%  You shoudl also survey results in the paper: e.g., later you write ``In paper II, we show that ... reduces to reachability ...''}
\closeparagraph{Verifying Linearization Policies} By using an observer to specify the sequential semantics of the
data structure, and defining controllers that specify the linearization policy,
the verification of linearizability is reduced to establishing four conditions:
\begin{inparaenum}[(i)]
\item
  each method invocation generates a non-empty sequence of operations,
\item
  the last operation of a method conforms to its parameters and return value,
\item
  only the last operation of a method may
  change the state of the observer,
  and
\item
  the sequence of all operations
  cannot drive the observer to an accepting state.
\end{inparaenum}
Our verification framework automatically reduces
the establishment of these conditions to 
a problem of checking control state reachability. This is done by
augmenting the observer by a {\em monitor}. The monitor is automatically
generated. It keeps track of 
%% The monitor 
the state of the observer, and records the sequence of operations and
call and return actions generated by the threads.
For each thread, it keeps track of whether
it has linearized, whether it has caused a state change in the observer, and
the parameters used in its last linearization. Using this information, it
goes to an error state whenever any of the above four conditions is violated.

\chapter{Shape Analysis}
Pointers and heap-allocated storage are features of all modern imperative programming languages.
%they are ignored in most formal treatments of the semantics of imperative programming languages
%because their inclusion complicates the semantics of assignment statements: 
They are the most complicated features of imperative programming language:
\bjcom{Should be in plural}
an assignment through a
pointer variable (or through a pointer-valued component \bjcom{better ``field''} of a record) may have large side effects on programs.
\bjcom{skip ``on programs''}
For example, dereferencing a pointer that has been freed will lead to segmentation faults \bjcom{use singular} in a C++ program.
\bjfix{C++}{C or C++}
There exist several works that have treated the semantics of pointers such as works in [5, 42, 43, 45].
\bjcom{This last sentence disturbs the flow. Change and move it til later}
These side effects also make program dependence \bjcom{Maybe skip ``dependence''} analysis harder, because they make it
difficult to compute the aliasing relationships among different pointer expressions in a program.
\bjcom{Before the last sentence, explain what is aliasing}
For instance, consider the instruction $\tt x.f := y$ written in an imperative language with mutable records.
Its effect is to assign the value of the pointer $\tt y$ to the  field f of the record pointed to by $\tt x$. We have to require information about all possible aliases of $\tt x$ in order to propagate the structural modification induced by the assignment.
\bjcom{This last sentence is not understandable. I suggest you rewrite and make it less complicated. Find the simplest way to explain what is here some problem}
Having less precise program dependence information
\bjcom{You often use ``program dependence information'' without explaining what it means}
decreases the opportunities for automatic parallelization
and for instruction scheduling. The usage of pointers is error prone.
\bjcom{This last thing was more or less said in the beginning of the page}
Dereferencing NULL pointers and accessing previously deallocated storage are two common programming mistakes. The usage of pointers in programs is thus an obstacle for
program understanding, debugging, and optimization. These activities need answers to many questions
about the structure of the heap contents and the pointer variables pointing into the heap. 
\bjcom{It is good to have an introductory paragraph, like this one, to give an idea of what are the challenges with pointers. But please rewrite it to have a more logical flow, and make the problems that you want to bring up clear}

\bjcom{A suggestion is to have: First a paragraph describing pointers and generla problems (aliasing, segmentation fault). A next paragraph can be to introduce that
  in verification and program analysis, it is a problem to describe and deduce how the heap-allocated memory is organized. E.g., Program invariants must often
  describe how the heap-allocated memoty is structured in order to infer the effects of statements that dereference pointer fields. This is the topic of
  ``shape analysis''}

By shapes, which mean descriptors of heap contents. Shape analysis is a generic term denoting static
program-analysis techniques that attempt to discover and verify properties of the heap contents in (usually imperative) computer programs. The shape analysis problem becomes more interesting in concurrent programs that manipulate pointers and dynamically allocated objects. \bjcom{Why is it more interesting in ``concurrent programs''} The area \bjfix{area}{problem} of verifying these programs has been a subject of intense research for quite some time. Currently, there are several competing approaches for symbolic heap abstraction. \bjcom{``symbolic heap abstraction'' is suddenly an unexplained term} The first approach is \bjcom{I do not think this was the first} based on the use of logics to present heap configurations. The logics can be separation logic \cite{John:SL, Stephen:SL,JoshCris:SL,Hongseok:SL,Kamil:SL,Chin:SL,Quang:SL, Ruzica:SL, Constrantin:SL}, 3-valued logic \cite{SagivRW02}, monadic second- order logic \cite{Ander:ML, Jakob:ML,Madhusudan:ML} or other \cite{Shmuel:Shape, Karen:Shape}. Another approach is based on the use of automata. In this approach, elements of languages of the automata describe configurations of the heap \cite{Ahmed:TreeAutomata, Ahmed:TreeAutomata2}. The last approach that we will mention is based on graph grammars describing heap graphs \cite{Jonathan:Shape, Jonathan:Grammars}. The presented approaches differ in their degree of specialisation for a particular class of data structures, their efficiency, and their level of dependence on user assistance (such as definition of loop invariants or inductive predicates for the considered data structures).
\bjcom{Break the preceding paragraph before you get to mentioning specific approaches (appr. around ``The logics can be'') You must then go on more slowly}
  
Among the works based on separation logic, the work, such as \cite{JoshCris:SL,Hongseok:SL, Quang:SL} proposed more efficient approaches. The reason for that is that their approaches effectively decomposes the heap into disjoint components and process them independently). However, most of the techniques based on separation logic are either specialised for some particular data structure, or they need to be provided inductive definitions of the data structures. In addition, their entailment checking procedures are either for specific class of data structures or based on folding/unfolding inductive predicates in the formulae and trying to obtain a syntactic proof of the entailment. 

This issue can be fixed by automata techniques using the generality of the automata-based representation such as techniques using tree automata. Finite tree automata, for instance, have been shown to provide a good balance between efficiency and expressiveness. The work \cite{Ahmed:TreeAutomata} uses a finite tree automaton to describe a set of heaps on a tree structure,
and represent non-tree edges by using regular “routing” expressions. These expressions
describe how the target can be reached from the source using tree edges. Finite tree transducers
are used to compute set of reachable configurations, and symbolic configuration is abstracted
collapsing certain states of the automat. The refinement technique called counterexample-guided
abstraction refinement (CEGAR) technique is used during the run of the analysis. This technique
is fully automatically and can handle complex data structures such as binary trees with linked
leaves. However, it suffers from the inefficiency and it also can not handle concurrent programs.

TVLA (Three-Valued Logic Analyzer) \cite{SagivRW02} is the first and one of the most popular shape analysis
method. It is based on a three-valued first-order predicate logic with transitive closure. Intuitively,
concrete heap structure is represented by a finite set of abstract summary nodes, each of them
representing a set of concrete nodes. The shape of the heap is characterized by a set of usersupplied
predicates. The method is not fully automatic, its the synthesis of appropriate predicates
that are able to express the invariants in the program. This problem is even more difficult with
complicated heap structures such as skiplist, trees, or lists of lists.  
  
\section*{Our Approaches}
In this thesis, we proposed three approaches for heap abstractions. In paper I, we proposed a novel approach of representing sets of heaps via tree automata (TA). In our representation, a heap is split in a canonical way into several tree components whose roots are the so-called cut-points. Cut-points are nodes pointed to by program variables or having several incoming edges. The tree components can refer to the roots of each other, and hence they are “separated” much like heaps described by formulae joined by the separating conjunction in separation logic [15]. Using this decomposition, sets of heaps with a bounded number of cut-points are then represented by the so called forest automata (FA) that are basically tuples of TA accepting tuples of trees whose leaves can refer back to the roots of the trees. Moreover, we allow alphabets of FA to contain nested FA, leading to a hierarchical encoding of heaps, allowing us to represent even sets of heaps with an unbounded number of cut-points (e.g., sets of DLL, skiplist). \input TA
In addition, we express relationships between data elements associated with nodes of the heap graph by two classes of constraints. Local data constraints are associated with transitions of TA and capture relationships between data of neighboring nodes in a heap graph; they can be used, e.g., to represent ordering internal to some structure such as a binary search tree. Global data constraints are associated with states of TA and capture relationships between data in distant parts of the heap. This approach was applied to verification of sequential heap manipulation programs. This approach is general and fully automatic, it can handle many types of sequential programs without any manual step. However, due to the complexity of tree automata operations, this approach is not suitable to handle concurrent programs where a large number of states and computation are needed. Figure \ref{figure:forest} shows an example of how to represent a heap by a set of tree automata. Figure \ref{figure:forest}(a) shows an example of a heap where nodes whose values are \nodea, \nodeb, \nodec, \noded, \nodee \; are cut-points, and $\tt x$, $\tt y$, $\tt z$ are local pointer variables and $\tt g$ is global pointer variable. Figure \ref{figure:forest}(b) shows its forest representation. In the forest representation, there are five TAs in which the TAs \taa \; and \tac \; refer to the root of the last TA \tae\;, and both TAs \tab \; and \tad \; refer to the root of TA \tac \;. The local data constraints are located along the solid arrows between nodes, whereas global constraints are located along the dashed arrows. In this figure, the global constraints $\tt \prec_{aa}$ means that all nodes in the left hand side are smaller than all nodes in the right hand side. We just show here small examples of data constraints, the detail about different types of constraints can be found in paper I.   

In paper II, we provide a symbolic encoding of the heap structure, that is less precise than the approach in paper I. However it is precise enough to allow the verification of the concurrent algorithms, and efficient enough to make the verification procedure feasible in practice. The main idea of the abstraction is to have a more precise description of the parts of the heap that are visible (reachable) from global variables, and to make a succinct representation of the parts that are local to the threads. More concretely, we will extract a set of heap segments, where the end points of a segment is pointed to by a cut-point which is reachable from global variables. A cut-point in this approach is a reachable node from global variable, and pointed by a global variables or having more than two incoming pointers. For each segment, we will store a summary of the content of the heap along the segment. This summary consists of two parts, each part contains different pieces of information, including the values of the cell variables if they have finite values, and the ordering among them if they are integer variables. The first part summaries information between the end point and its predecessor, whereas the second part summaries information between the start point and the predecessor node. For each given program, the set of possible abstract shapes insight and hence the verification procedure is guaranteed to terminate. This approach is very efficient but it is not optimal for complicated concurrent data structures like trees, lists of lists or skiplists. Figure \ref{heapsummary} gives our summary abstraction of the heap in figure \ref{figure:forest}(a). In this approach, \nodea, \nodeb, \nodec, \nodee \; are cut-points. The node \noded \; is not a cut-point like the approach in paper I because its not reachable from the global variable $\tt g$. In each heap segment, the fist part is described by the white box, and the second part is described by the gray box.   
\input SL
In paper III, we present an approach which can handle concurrent programs implemented from simple to complex data structures. In our fragment abstraction, we represent the part of the heap that is accessible to a thread by a set of fragments. A fragment represents a pair of heap cells (accessible to $\thread$)
that are connected by a pointer field, under the applied data abstraction. The fragment contains both
(i) {\em local} information about the cell's fields and variables that
  point to it, as well as
(ii) {\em global} information, representing how
  each cell in the pair can reach to and be reached from
  (by following a chain of pointers) a small set of globally significant
  heap cells.
 A set of fragments represents the set of heap
structures in which each pair of pointer-connected nodes is represented by some
fragment in the set.
Put differently, a set of fragments describes the set of heaps that can be formed by
``piecing together'' pairs of pointer-connected nodes that are represented
by some fragment in the set. This ``piecing together'' must
be both locally consistent (appending only fragments that agree on their
common node), and globally consistent (respecting the global reachability
information).
\input fragment

Let us illustrate how pairs of heap nodes can be represented by fragments. Figure \ref{fragment} shows the set of fragments abstracted from the heap in \ref{figure:forest}(a). In each fragment, the ordering between two keys of two nodes is shown as a label on the arrow between two tags. Above each tag is pointer variables. The first brown row under each tag is $\tt reachfrom$ information, whereas the second green row is $\tt reachto$ information.

%To verify linearizability of the algorithm in Figure~\ref{figure:lazy-list},
%we must represent several key invariants of the heap. These include (among others)
%\begin{numberedlist}
%	\item The list is strictly sorted in $\tt key$ order, two unmarked nodes cannot have the same $\tt key$.
%\item All nodes which are unreachable from the head of the list are marked.
%\item The variable $\tt p$ points to a cells whose $\tt key$ field is never
%  larger than the input parameter of its $\tt add$,$\tt rmv$ and $\tt cnt$ methods.
%\end{numberedlist}
%Let us illustrate how such invariants are captured by our fragment abstraction. 1) All fragments are strictly sorted, implying that the list is strictly sorted. 2) This is verified by inspecting each tag: $\frag_{6}$ contains the only unreachable tag, and it is also marked. 3) The fragments express this property in the case where the value of $\tt key$ is the same as the value of the observer register $\tt x$. Since the invariant holds for any value of $\tt x$, this property is sufficiently represented for purposes of verification.   




